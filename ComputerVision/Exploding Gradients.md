The exploding gradient problem typically occurs when gradients are multiplied through many layers during backpropagation. In deep neural networks, especially those with many layers or complex architectures, gradients can accumulate and grow exponentially as they propagate backwards through the layers. This explosion in gradient size can cause weight updates to become too large, leading to erratic training behaviour such as oscillation or divergence.