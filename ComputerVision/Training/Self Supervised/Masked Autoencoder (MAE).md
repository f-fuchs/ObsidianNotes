# Masked Autoencoder (MAE)

Masked autoencoders (MAE) are scalable self-supervised learners for computer vision with a simple approach: mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), and a lightweight decoder that reconstructs the original image from the latent representation and mask tokens.

![[masked_autoencoders.png]]

## Masking

Following [[Vision Transformer|ViT]], the image is first divided into regular non-overlapping patches of size16x16 and then a token is generated by linear projection with added positional embedding from each patch. Next, these tokens are randomly shuffled and the last part of the list is removed based on the masking ratio (i.e., the ratio of patches removed).

A ratio of 75% has been found to be good for both linear probing and fine tuning. A high masking ratio largely eliminates redundancy, creating a task that cannot be easily solved by extrapolation from visible neighboring patches, and instead promotes the learning of useful representations. Finally, the highly sparse input provides an opportunity to design an efficient encoder.

## Encoder

The encoder is a *vanilla* [[Vision Transformer|ViT]] that is only applied to unmasked patches (e.g., 25% of the full set). This allows very large encoders to be trained with a fraction of the computation and memory.

## Decoder

The decoder can be designed independently of the encoder design. What is required is that the input to the decoder is the full set of tokens consisting of (i) encoded visible patches and (ii) mask tokens. Each mask token is a shared, learned vector indicating the presence of a missing patch to be predicted. Before the full list of tokens is passed through the transformer blocks, the list is unshuffled and positional embeddings are added. Without this, the mask tokens would have no information about their location in the image.

The number of transformer blocks in the decoder depends on the task. While a sufficiently deep decoder is important for linear probing, a single block decoder can perform well with ﬁne tuning. Note that a single transformer block is the minimum requirement for propagating information from visible tokens to mask tokens. The final decoder has eight transformer blocks and thus <10% computation per token compared to the ViT-L encoder with its 24 blocks. This asymmetric design, where the full set of tokens is only processed by the lightweight decoder, significantly reduces pre-training time.

The final layer of the decoder is a linear projection whose number of output channels is equal to the number of pixel values in a patch. The output of the decoder is then transformed into a reconstructed image.

## Reconstruction Target

The loss function computes the mean square error (MSE, L2) between the reconstructed and patch-normalized (computing the mean and standard deviation of all pixels in a patch and using them to normalize that patch) original images in pixel space.

## Data Augmentation

MAE works well using cropping-only augmentation, either ﬁxed-size or random-size (both having random horizontal ﬂipping). Adding color jittering even degrades the results. Surprisingly, MAE behaves decently even if using no data augmentation (only center-crop, no ﬂipping). This property is dramatically different from contrastive learning and related methods that rely heavily on data augmentation. In MAE, the role of data augmentation is mainly performed by random masking. The masks are different for each iteration and so they generate new training samples regardless of data augmentation. The pretext task is made difﬁcult by masking and requires less augmentation to regularize training.

## Resources

- [[2111.06377] Masked Autoencoders Are Scalable Vision Learners (arxiv.org)](https://arxiv.org/abs/2111.06377)
- [GitHub - facebookresearch/mae: PyTorch implementation of MAE https//arxiv.org/abs/2111.06377](https://github.com/facebookresearch/mae)
