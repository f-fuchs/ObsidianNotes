---
dg-publish: true
---

# Fully Convolutional Masked Autoencoder (FCMAE)

FCMAE are an adaptation of [[Masked Autoencoder (MAE)|MAE]]s for [[ConvNeXt]] models that run in a fully convolutional manner. Similar to [[Masked Autoencoder (MAE)|MAE]]s the learning signals are generated by randomly masking the raw input visuals with a high masking ratio and letting the model predict the missing parts given the remaining context.

![[fcmae.png|700]]

## Masking

As the convolutional model has a hierarchical design, where the features are downsampled in different stages, the mask is generated in the last stage and upsampled recursively up to the finest resolution. To implement this in practice, we randomly remove 60% of the 32 Ã— 32 patches from the original input image. Only random resized cropping is used as data augmentation.

## Encoder Design

One challenge in masked image modeling is preventing models from copying and pasting information from masked regions. In transformer-based models, this is achieved by using only visible patches as input. However, for ConvNets, preserving the 2D image structure complicates this, and naive solutions that introduce learnable masked tokens reduce pretraining efficiency and create inconsistencies between training and testing. Instead the masked image can be represented as a 2D sparse pixel array. By converting standard convolution layers to sparse convolution ones during pre-training, the model operates only on visible data points. During fine-tuning sparse convolution layers can revert to standard convolution ones without extra handling. Alternatively, a binary masking operation before and after dense convolution can achieve similar effects.

## Decoder Design

The decoder consist of just a plain [[ConvNeXt#ConvNeXt Block|ConvNeXt block]] with dimension 512. This forms an asymmetric encoder decoder architecture overall, as the encoder is heavier and has a hierarchy. More complex decoder design were considered but did not offer any advantages, see tables below.

![[fcmae_decoder.png]]

> [!question]
> Relationship between decoder depth and linear probing performance? For [[Masked Autoencoder (MAE)]] one block seems fine for fine tuning but more are needed for linear probing.

## Reconstruction Target

Similar to [[Masked Autoencoder (MAE)|MAE]], the target is a patch-wise normalized image of the original input, and the loss is applied only on the masked patches. The mean squared error (MSE) between the reconstructed and target images is used as loss.

## Resources

- [https://github.com/facebookresearch/ConvNeXt-V2](https://github.com/facebookresearch/ConvNeXt-V2)
